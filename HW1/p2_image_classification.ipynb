{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true,"authorship_tag":"ABX9TyM4aiYVsb0YuikYku5T5McD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"kj-EphJ8br_q"}},{"cell_type":"code","source":["# Install the necessary Python packages\n","!pip install numpy\n","!pip install tqdm\n","!pip install torch\n","!pip install torchvision\n","!pip install matplotlib\n","!pip install Pillow"],"metadata":{"id":"xZDD4NnxNMK7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import argparse, pdb\n","import numpy as np\n","import os\n","import tqdm\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, TensorDataset, DataLoader\n","from PIL import Image\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","from typing import Callable\n","import matplotlib.pyplot as plt"],"metadata":{"id":"CbOOpTfzbvNK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download image dataset from Google Drive\n","! pip install gdown\n","! gdown 1Oeto_5xV_l4zVJIi1fFDsTrzE3y8mKUK\n","! unzip csci699_p2_dataset.zip"],"metadata":{"id":"U-j_kupTU0vn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Section 1: Generate embedding dataset"],"metadata":{"id":"NyV4b9Odbvox"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"o-fuTlA6TIXx"},"outputs":[],"source":["# Define some useful hyperparameters\n","IMG_SIZE = 299\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","LABELS = [\"cat\", \"dog\", \"neg\"]\n","\n","train_image_dir = \"/content/datasets/train\"\n","test_image_dir = \"/content/datasets/test\"\n","\n","transform = transforms.Compose([\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),            # Resize to dimensions for Inception network\n","    transforms.ToTensor(),                              # Convert PIL Image to tensor\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n","])\n","\n","# See https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html for\n","# more details on usage.\n","train_img_folder = torchvision.datasets.ImageFolder(root=train_image_dir, transform=transform)\n","\n","def to_numpy(tensor):\n","  return tensor.detach().cpu().numpy()\n","\n","train_embs, train_labels = [], []\n","print('number of train images: ', len(train_img_folder))\n","\n","######### Your code starts here #########\n","# We want to create a dataset of Inception-v3 embeddings.\n","# Hint: You can use the pretrained PyTorch Inception model here (https://pytorch.org/hub/pytorch_vision_inception_v3/)\n","# Iteration through the training images, use the bottleneck layers of Inception to generate embeddings\n","# and appending them to the respective lists: train_embs and train_labels\n","\n","######### Your code ends here #########\n","\n","train_embs = torch.from_numpy(np.concatenate(train_embs))\n","train_labels = torch.Tensor(train_labels)\n","train_dataset = TensorDataset(train_embs, train_labels)"]},{"cell_type":"markdown","source":["# Section 2: Train linear classifier"],"metadata":{"id":"YbjXJkHtaMzM"}},{"cell_type":"code","source":["# Define some training hyperparameters, feel free to modify these\n","num_epochs = 20\n","lr = 1e-3\n","batch_size = 64\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","######### Your code starts here #########\n","# We want to create a linear classifier which takes the embedding vectors as input.\n","# The training loop is already provided for you.\n","\n","# 1. Define a new torch module for the classifier.\n","# 2. Define an appropriate optimizer from torch.optim.\n","# 3. Define the loss function for training the classifier.\n","\n","linear_classifier =\n","optimizer =\n","loss_fn =\n","######### Your code ends here #########\n","\n","\n","for epoch in range(num_epochs):\n","  # set model to training mode\n","  linear_classifier.train()\n","  train_loss = 0\n","\n","  for batch_idx, (embeddings, class_label) in enumerate(train_loader):\n","    embeddings = embeddings.to(device)\n","    class_label = class_label.to(device)\n","\n","    optimizer.zero_grad()\n","\n","    probs = linear_classifier(embeddings)\n","    loss = loss_fn(probs, class_label.long())\n","\n","    loss.backward()\n","    train_loss += loss.item()\n","    optimizer.step()\n","\n","  print('Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader)))"],"metadata":{"id":"YEANPpn9bLO2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Section 3: Classify test images"],"metadata":{"id":"kCLQoCnYhupi"}},{"cell_type":"code","source":["transform = transforms.Compose([\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),            # Resize to dimensions for Inception network\n","    transforms.ToTensor(),                              # Convert PIL Image to tensor\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n","])\n","\n","# Load the test dataset\n","test_dataset = torchvision.datasets.ImageFolder(root=test_image_dir, transform=transform)\n","\n","######### Your code starts here #########\n","# Classify all images in the test image folder\n","# Calculate the accuracy of the model on all of the images.\n","\n","test_acc =\n","######### Your code ends here ########\n","\n","print(f\"Evaluated on {len(test_dataset)} samples.\")\n","print(f\"Accuracy: {test_acc * 100:.0f}%\")"],"metadata":{"id":"ek2vJo3fcWpc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Section 4: Object Detection"],"metadata":{"id":"5iRJHisqlrE7"}},{"cell_type":"code","source":["def compute_brute_force_classification(\n","    model: nn.Module,\n","    raw_img: np.ndarray,\n","    transforms,\n","    nH: int = 8,\n","    nW: int = 8\n","):\n","    '''\n","    This function returns the probabilities of each window.\n","    Inputs:\n","        model: Model which is used\n","        raw_img: H x W x 3 numpy array\n","        transforms: a sequence of transformations to apply to the image as preprocessing\n","        nH: number of windows in the vertical direction\n","        nW: number of windows in the horizontal direction\n","    Outputs:\n","        window_predictions: a (nH, nW, 3) np.array.\n","                            The last dim (size 3) is the probabilities\n","                            of each label (cat, dog, neg)\n","    '''\n","    ######### Your code starts here #########\n","\n","\n","    ######### Your code ends here #########\n","\n","    return window_predictions"],"metadata":{"id":"BPOMeOhMZbrV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_classification(raw_img, classification_array):\n","    nH, nW, _ = classification_array.shape\n","    aspect_ratio = float(raw_img.shape[0]) / raw_img.shape[1]\n","    plt.figure(figsize=(8, 8*aspect_ratio))\n","    p1 = plt.subplot(2,2,1)\n","    plt.imshow(classification_array[:,:,0], interpolation='none', cmap='jet')\n","    plt.title('%s probability' % LABELS[0])\n","    p1.set_aspect(aspect_ratio*nW/nH)\n","    plt.colorbar()\n","    p2 = plt.subplot(2,2,2)\n","    plt.imshow(classification_array[:,:,1], interpolation='none', cmap='jet')\n","    plt.title('%s probability' % LABELS[1])\n","    p2.set_aspect(aspect_ratio*nW/nH)\n","    plt.colorbar()\n","    p2 = plt.subplot(2,2,3)\n","    plt.imshow(classification_array[:,:,2], interpolation='none', cmap='jet')\n","    plt.title('%s probability' % LABELS[2])\n","    p2.set_aspect(aspect_ratio*nW/nH)\n","    plt.colorbar()\n","    plt.subplot(2,2,4)\n","    plt.imshow(raw_img)\n","    plt.savefig(\"detect.png\")\n","    plt.show()"],"metadata":{"id":"djESqZV8Zz6p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get raw image using PIL\n","catswithdogs_dir = \"/content/datasets/catswithdogs\"\n","raw_img = np.array(Image.open(os.path.join(catswithdogs_dir, \"001211.jpg\")))\n","\n","# Define transformations\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),                  # Numpy array to PIL Image first\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),  # Resize to dimensions for Inception network\n","    transforms.ToTensor(),                    # Convert PIL Image to tensor\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n","])\n","\n","window_predictions = compute_brute_force_classification(\n","    model= # TODO: fill this in,\n","    raw_img=raw_img,\n","    transforms=transform,\n",")\n","\n","plot_classification(raw_img, window_predictions)"],"metadata":{"id":"qp37Zt-GbX-u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_convolutional_KxK_classification(\n","    model: nn.Module,\n","    raw_img: np.ndarray,\n","    transforms\n","):\n","    \"\"\"\n","    Computes probabilities for each window based on the convolution layer of Inception\n","    Inputs:\n","      model: model which is used\n","      raw_img: numpy array of image\n","\n","    Outputs:\n","      predictions: a (K, K, 3) np.array.\n","    \"\"\"\n","    transformed_img = transform(raw_img)\n","\n","    ######### Your code starts here #########\n","\n","    ######### Your code ends here #########\n","\n","    return predictions"],"metadata":{"id":"NjcexUiPZjrE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get raw image, do not apply transform\n","catswithdogs_dir = \"/content/datasets/catswithdogs\"\n","raw_img = np.array(Image.open(os.path.join(catswithdogs_dir, \"001211.jpg\")))\n","\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),                  # Numpy array to PIL Image first\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),  # Resize to dimensions for Inception network\n","    transforms.ToTensor(),                    # Convert PIL Image to tensor\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n","])\n","\n","window_predictions = compute_convolutional_KxK_classification(\n","    model= # TODO: fill this in,\n","    raw_img=raw_img,\n","    transforms=transform,\n",")\n","\n","plot_classification(raw_img, window_predictions)"],"metadata":{"id":"1pYHbY-ye1iU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Section 5: Saliency Mapping"],"metadata":{"id":"zR0Lv_vUogSM"}},{"cell_type":"code","source":["def compute_and_plot_saliency(\n","    model: nn.Module,\n","    raw_img: np.ndarray\n","):\n","    \"\"\"\n","    This function computes and plots the saliency plot.\n","    You need to compute the matrix M detailed in section 3.1 in\n","    K. Simonyan, A. Vedaldi, and A. Zisserman,\n","    \"Deep inside convolutional networks: Visualising imageclassification models and saliency maps,\"\n","    2013, Available at https://arxiv.org/abs/1312.6034.\n","​\n","    Inputs:\n","      model: model which is used\n","      raw_img: numpy array of image\n","    \"\"\"\n","    ######### Your code starts here #########\n","\n","    ######### Your code ends here #########\n","\n","    # Code to save the saliency plot\n","    plt.subplot(2, 1, 1)\n","    plt.imshow(M)\n","    plt.title('Saliency with respect to predicted class %s' % LABELS[top_class])\n","    plt.subplot(2, 1, 2)\n","    plt.imshow(raw_img)\n","    plt.savefig(\"saliency.png\")\n","    plt.show()\n"],"metadata":{"id":"MnrYhV1QZxlc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get raw image, do not apply transform\n","catswithdogs_dir = \"/content/datasets/catswithdogs\"\n","raw_img = np.array(Image.open(os.path.join(catswithdogs_dir, \"001211.jpg\")))\n","\n","compute_and_plot_saliency(\n","    model = # TODO: fill this in,\n","    raw_img = raw_img\n",")"],"metadata":{"id":"OlTZSeYxRdiz"},"execution_count":null,"outputs":[]}]}
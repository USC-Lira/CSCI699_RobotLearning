{"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"7mnIg8H_3FNc"}},{"cell_type":"code","source":["# Install the necessary Python packages\n","!pip install numpy\n","!pip install tqdm\n","!pip install torch\n","!pip install torchvision\n","!pip install matplotlib\n","!pip install Pillow"],"metadata":{"id":"nT_fkacVTrVY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prerequisites\n","import torch\n","import torch.utils.data\n","from torch.utils.data import DataLoader\n","from torch import nn\n","import torch.nn.functional as F\n","from torch import optim\n","\n","import torchvision\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image\n","\n","from tqdm import tqdm\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from typing import Callable"],"metadata":{"id":"1zkdfhCo2nYA","executionInfo":{"status":"ok","timestamp":1693548462298,"user_tz":240,"elapsed":2,"user":{"displayName":"Anthony Liang","userId":"02870980873029502886"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Download Dataset"],"metadata":{"id":"ycPwI9wF6A_6"}},{"cell_type":"code","source":["bs = 100\n","# MNIST Dataset\n","train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n","test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n","\n","# Data Loaders\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"],"metadata":{"id":"xgO0HApX6AKN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Section 1: Autoencoder"],"metadata":{"id":"z7vVcnfB3LjG"}},{"cell_type":"code","source":["class Autoencoder(nn.Module):\n","\n","  def __init__(\n","      self,\n","      input_dim: int = 784,\n","      hidden_dim: int = 256,\n","      device: str = \"cuda\"\n","  ):\n","    super(Autoencoder, self).__init__()\n","    self.device = device\n","\n","    ######### Your code starts here #########\n","    # Encoder architecture:\n","    # 2 fully connected layers with ReLU activations\n","\n","    # Decoder architecture:\n","    # 3 fully connected layers with ReLU activations\n","\n","    # Choose the correct final layer output activation\n","\n","    self.encoder =\n","\n","    self.decoder =\n","    ######### Your code ends here #########\n","\n","  def forward(self, x):\n","    latent = self.encoder(x)\n","    reconstruction = self.decoder(latent)\n","    return reconstruction"],"metadata":{"id":"9X1ahmBEDvBb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Section 2: Variational Autoencoder"],"metadata":{"id":"hwD83wMZUR3l"}},{"cell_type":"code","source":["class VAE(nn.Module):\n","\n","  def __init__(self, input_dim: int = 784, hidden_dim: int = 256, latent_dim: int = 2, device: str = \"cuda\"):\n","    super(VAE, self).__init__()\n","    self.latent_dim = latent_dim\n","    self.device = device\n","\n","    ######### Your code starts here #########\n","    # Encoder architecture:\n","    # 2 fully connected layers with ReLU activations\n","\n","    # Decoder architecture:\n","    # 3 fully connected layers with ReLU activations\n","\n","    # Separate linear layers to predict the latent mean and logvar\n","\n","    # Choose the correct final layer output activation\n","\n","    self.encoder =\n","\n","    self.decoder =\n","\n","    self.mean =\n","\n","    self.log_var =\n","    ######### Your code ends here #########\n","\n","  def reparameterize(self, mu, log_var):\n","    std = torch.exp(0.5 * log_var)\n","    eps = torch.randn_like(std)\n","    return eps.mul(std).add_(mu)\n","\n","  def sample(self, n):\n","    sample = torch.randn(n, self.latent_dim).to(self.device)\n","    return self.decoder(sample)\n","\n","  def forward(self, x):\n","    ######### Your code starts here #########\n","\n","    ######### Your code ends here #########\n","    return reconstruction, mu, log_var"],"metadata":{"id":"33sPuYAq2m5t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define training variables, feel free to modify these for the problem\n","log_interval = 100\n","num_epochs = 10\n","image, cls = train_dataset[0]\n","input_dim = np.product(image.shape)\n","hidden_dim = 256\n","latent_dim = 2\n","batch_size = 100\n","num_examples = len(train_dataset)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","def train_single_epoch(\n","    model_cls: str,\n","    model: nn.Module,\n","    loss_fn: Callable,\n","    data_loader: DataLoader,\n","    optimizer,\n","    epoch: int\n","):\n","  # set model to training mode\n","  model.train()\n","  train_loss = 0\n","  for batch_idx, (data, class_label) in enumerate(data_loader):\n","    data = data.to(device)\n","    class_label = class_label.to(device)\n","\n","    optimizer.zero_grad()\n","\n","    if model_cls == \"ae\":\n","      recon_batch = model(data)\n","      # compute loss\n","      loss = loss_fn(recon_batch, data)\n","    elif model_cls == \"vae\":\n","      recon_batch, mu, log_var = model(data)\n","      # compute loss\n","      loss = loss_fn(recon_batch, data, mu, log_var)\n","    elif model_cls == \"cvae\":\n","      recon_batch, mu, log_var = model(data, class_label)\n","      # compute loss\n","      loss = loss_fn(recon_batch, data, mu, log_var)\n","    else:\n","      raise NotImplementedError\n","\n","    loss.backward()\n","    train_loss += loss.item()\n","    optimizer.step()\n","\n","    if batch_idx % log_interval == 0:\n","      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","            epoch, batch_idx * len(data), len(data_loader.dataset),\n","            100. * batch_idx / len(data_loader), loss.item() / len(data)))\n","\n","  print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(data_loader.dataset)))\n","\n","  # Plot reconstructions\n","  n = min(num_examples, 8)\n","  imgs = data[:n]\n","  reconstructed_imgs = recon_batch.view(batch_size,-1, 28, 28)[:n]\n","\n","  comparisons = torch.cat([imgs, reconstructed_imgs])\n","\n","  # sample some from the latent space, with class label 0\n","  if model_cls in [\"vae\", \"cvae\"]:\n","    latent_samples = model.sample(n=n)\n","    latent_samples = latent_samples.view(n, -1, 28, 28)\n","    comparisons = torch.cat([comparisons, latent_samples])\n","\n","  comparisons = torchvision.utils.make_grid(comparisons)\n","  comparisons = comparisons.detach().cpu().numpy()\n","\n","  print(\"Reconstructions: \")\n","  plt.imshow(comparisons.transpose(1,2,0))\n","  plt.axis('off')\n","  plt.show()"],"metadata":{"id":"7t-jAted2uUC","executionInfo":{"status":"ok","timestamp":1693548467704,"user_tz":240,"elapsed":8,"user":{"displayName":"Anthony Liang","userId":"02870980873029502886"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# Section 3: Train Autoencoder"],"metadata":{"id":"50fY_Slp0dEP"}},{"cell_type":"code","source":["# build model\n","ae = Autoencoder(\n","  input_dim=input_dim,\n","  hidden_dim=hidden_dim,\n","  device=device\n",")\n","\n","# put model on device\n","if torch.cuda.is_available():\n","  ae.cuda()\n","\n","# device optimizer\n","optimizer = optim.Adam(ae.parameters())\n","\n","######### Your code starts here #########\n","# Define the loss function for a vanilla Autoencoder.\n","loss_fn =\n","######### Your code ends here #########\n","\n","# train\n","for epoch in range(1, num_epochs):\n","  train_single_epoch(\n","    model_cls=\"ae\",\n","    model=ae,\n","    loss_fn=loss_fn,\n","    data_loader=train_loader,\n","    optimizer=optimizer,\n","    epoch=epoch\n","  )"],"metadata":{"id":"lqNyR1AkU0wZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Section 4: Train Variational Autoencoder"],"metadata":{"id":"IbIeNvZI1_Ac"}},{"cell_type":"code","source":["# build model\n","vae = VAE(\n","  input_dim=input_dim,\n","  hidden_dim=hidden_dim,\n","  latent_dim=latent_dim,\n","  device=device\n",")\n","\n","# put model on device\n","if torch.cuda.is_available():\n","  vae.cuda()\n","\n","# device optimizer\n","vae_optimizer = optim.Adam(vae.parameters())\n","\n","######### Your code starts here #########\n","# Define the loss function for a vanilla Autoencoder.\n","vae_loss_fn =\n","######### Your code ends here #########\n","\n","for epoch in range(1, num_epochs):\n","  train_single_epoch(\n","    model_cls=\"vae\",\n","    model=vae,\n","    loss_fn=vae_loss_fn,\n","    data_loader=train_loader,\n","    optimizer=vae_optimizer,\n","    epoch=epoch\n","  )"],"metadata":{"id":"rw_2QEmv2Arn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Bonus task: Conditional VAE"],"metadata":{"id":"gJ5BcC5H-FEM"}},{"cell_type":"code","source":["class ConditionalVAE(nn.Module):\n","\n","  def __init__(\n","      self,\n","      input_dim: int,\n","      hidden_dim: int = 256,\n","      latent_dim: int= 2,\n","      embedding_dim: int = 64,\n","      num_classes: int = 10,\n","      device: str = \"cuda\"\n","  ):\n","    super(ConditionalVAE, self).__init__()\n","\n","    self.latent_dim = latent_dim\n","    self.device = device\n","\n","    ######### Your code starts here #########\n","    # Encoder architecture:\n","    # 2 fully connected layers with ReLU activations\n","\n","    # Decoder architecture:\n","    # 3 fully connected layers with ReLU activations\n","\n","    # Separate linear layers to predict the latent mean and logvar\n","\n","    # Use an embedding layer to encode the class label.\n","    # See https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html.\n","\n","    # Choose the correct final layer output activation\n","\n","    self.label_embed =\n","\n","    self.encoder =\n","\n","    self.decoder =\n","\n","    self.mean =\n","\n","    self.log_var =\n","    ######### Your code ends here #########\n","\n","  def reparameterize(self, mu, log_var):\n","    std = torch.exp(0.5 * log_var)\n","    eps = torch.randn_like(std)\n","    return eps.mul(std).add_(mu)  # return z sample\n","\n","  def sample(self, n, class_label=0):\n","    sample = torch.randn(n, self.latent_dim).to(self.device)\n","    class_label = torch.tensor(class_label).repeat(n).to(self.device)\n","    return self.decode(sample, class_label)\n","\n","  def decode(self, x, class_label):\n","    # embed the class label\n","    label_embed = self.label_embed(class_label)\n","    decoder_input = torch.cat([x, label_embed], dim=-1)\n","    z = self.decoder(decoder_input)\n","    return z\n","\n","  def encode(self, x, class_label):\n","    # embed the class label\n","    label_embed = self.label_embed(class_label)\n","\n","    encoder_input = torch.cat([x.view(x.shape[0], -1), label_embed], dim=-1)\n","    encode_output = self.encoder(encoder_input)\n","\n","    mu = self.mean(encode_output)\n","    log_var = self.log_var(encode_output)\n","    return mu, log_var\n","\n","  def forward(self, x, class_label):\n","    mu, log_var = self.encode(x, class_label)\n","    z = self.reparameterize(mu, log_var)\n","    return self.decode(z, class_label), mu, log_var"],"metadata":{"id":"Y0i9Uzea-GAs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Bonus task: Train CVAE"],"metadata":{"id":"6uhpLHON2_p_"}},{"cell_type":"code","source":["# build model\n","cvae = ConditionalVAE(\n","  input_dim=input_dim,\n","  hidden_dim=hidden_dim,\n","  latent_dim=latent_dim,\n","  num_classes=10,\n","  device=device\n",")\n","\n","# put model on device\n","if torch.cuda.is_available():\n","  cvae.cuda()\n","\n","# device optimizer\n","cvae_optimizer = optim.Adam(cvae.parameters())\n","\n","for epoch in range(1, num_epochs):\n","  train_single_epoch(\n","    model_cls=\"cvae\",\n","    model=cvae,\n","    loss_fn=vae_loss_fn,\n","    data_loader=train_loader,\n","    optimizer=cvae_optimizer,\n","    epoch=epoch\n","  )"],"metadata":{"id":"Xgmmw8EY23jN"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"toc_visible":true,"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}